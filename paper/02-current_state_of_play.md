# The current state of play

Draft: Currently a lot of scientific research is not being digitised, and there are a multitude of reasons, and we are still lacking the standards to make this an easy process. Give some examples, e. g. supplemental data being pdf. Or take NMR, which is a digital technique from the start, how many data end up in digital repositories? Even excluding measurements which are purely for experimental control, how many of the new compounds published are properly documented? An example (do we have more?) for the better: CCDC

## 2.1. Why are people not digitising

There are a number of reasons why digitisation is in many cases not happening, even though the willingness exists. The main reasons we have identified fall in the following groups: (TODO can we find references?)

Some reasons evolve round personal issues. A major worry is the loss of control of data. This includes fears that data may be used by other researchers without proper credit being given. More fundamental is the fear to give away results and future research topics, when making data available. On the opposite end, some researchers feel their data are not good enough to be seen by others, and want to avoid being “shamed” for bad results.

Related to the fear of loss of data is the fear of not getting proper credit, which in an academic context means citations. Even if databases are properly acknowledged, this does not normally mean a citation for the data provider. This is an aspect of a wider problem in academic publishing, where there is only the concept of “the author” for papers, whereas research is these days dominated by division of labour.

A second group of issues evolves around lack of or perceived lack of standard formats. In some fields, there are a variety of standards to use, and it is unclear which one might be best for digitising. TODO examples here? Like NMR? In detail, the lack of standards can be broken down in two issues: Firstly, which data are actually digitising? This is particularly relevant in techniques where data are processed in several stages, where a researcher needs to decide at which level data gets digitised and archived. Secondly, for each level or type of data there can be competing standards, for example from device manufacturers, database providers, or processing software companies. Researchers afraid to do something wrong might choose to do nothing. 

The issue of competing standards is in some cases due to a disconnect between developers and lab chemists. The same is partly true for tools. The usability of tools and standards is therefore in some cases not as good as it could be, and requires additional time from chemists, which they are not willing to spend. TODO give examples? Or best practices?

In an academic context, enforcement from top often is either not possible or considered inappropriate. Group leaders or others who could prescribe something also face a difficult choice here and risk spending time and money on non-sustainable efforts, so they may shy away from taking a risk.  So a centralized approach is typically not an answer. In contrast, industry does typically enforce standards, but would be reluctant to make either data available and/or use open and transparent standards.
Even if those problems are overcome, there is still the issue of digitisation not being part of the standard workflow and causing extra work, or is perceived as causing extra work. Most researchers do not have time to do extra work. Finally, they may not be convinced that the efforts pay, since if formats or tools are badly chosen, digital data may not be available long term. This leads to the question of the  efficiency of current efforts.

- Why is it not happening even today after x years being in the making?
- Add a summary of the challenges in the etherpad here
- Has science reached a plateau in open data? Is it accessible (normally), machine-readable (sometimes) and machine-actionable (rarely)? 


## 2.2. Why is the current digitisation not effective

Apart from barriers to start digitisation, there is also a range of problems with the current state of digitisation, some related to the problems mentioned above.
Many of those problems can be summarized as imperfect or flawed digitisation efforts. In most cases those problems are of course the consequences of decisions which at some point seemed perfectly sensible, but turned out to give major problems later. A typical example for these is the adoption of a format which later fell out of use. The same is true about many other aspects of technology. Even though these problems are partly overcome today, it is still an issue. An example of an effort to digitise chemistry would be the attempt to put chemistry data on Second Life, once a thriving platform, which at some point was supported e.g. by Nature.
Now use bererslee to show what it should be like.
- Lack of standards, issues with ELNS
- Needs “3 pillars”: standards, streamlined data entry and repositories
- Papers are becoming “machine-readable” via proliferation of language models, but are these accurate enough for “machine-actionable” papers?

## 2.3. What factors will drive change?

- Digital “upskilling” of scientists generally
- Software tools becoming more accessible and reproducible (open source, containers, proliferation of cloud interfaces)
-  The science itself. No longer do people do just one experiment, larger science means more data. (Which is obvious, but worth saying that it’s somewhat inevitable, so they might as well engage now!).
